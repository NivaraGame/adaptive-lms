# –¢—Ä–∏ –†—ñ–≤–Ω—ñ –ú–µ—Ö–∞–Ω—ñ–∑–º—É –ê–¥–∞–ø—Ç–∞—Ü—ñ—ó

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ë–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–æ—ó –ê–¥–∞–ø—Ç–∞—Ü—ñ—ó

```mermaid
graph TB
    UP[User Profile]
    RM[Recent Metrics]
    SC[Session Context]

    UP --> SE[Strategy Executor]
    RM --> SE
    SC --> SE

    SE -->|rules| LA[Level A: Rules-Based]
    SE -->|bandit| LB[Level B: Contextual Bandit]
    SE -->|policy| LC[Level C: IRT/BKT Policy]

    LA --> R1[Threshold Rules]
    LA --> R2[Format Selection]
    LA --> R3[Tempo Logic]
    R1 --> DA[Decision A]
    R2 --> DA
    R3 --> DA

    LB --> FE[Feature Extractor]
    FE --> UCB[LinUCB Algorithm]
    UCB --> DB[Decision B]
    RW[Reward] -.->|update| UCB

    LC --> IRT[IRT Model]
    LC --> BKT[BKT Model]
    IRT --> KS[Knowledge State]
    BKT --> KS
    KS --> DC[Decision C]

    DA --> REC[Recommendation Service]
    DB --> REC
    DC --> REC

    REC --> CS[Content Selection]
    CS --> CONT[Selected Content]

    CONT --> USER[User Interaction]
    USER --> MET[Metrics]
    MET --> AGG[Aggregation]
    AGG --> UP
    MET -.-> RW

    style LA fill:#90EE90
    style LB fill:#FFD700
    style LC fill:#87CEEB
```

## –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ –¢–∞–±–ª–∏—Ü—è –†—ñ–≤–Ω—ñ–≤

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Level A: Rules | Level B: Bandit | Level C: IRT/BKT |
|----------------|----------------|-----------------|------------------|
| **–°—Ç–∞—Ç—É—Å** | ‚úÖ –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ | üîÑ –ü–ª–∞–Ω—É—î—Ç—å—Å—è Week 7 | üîÆ –ú–∞–π–±—É—Ç–Ω—î |
| **–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å** | –ü—Ä–æ—Å—Ç–∞ | –°–µ—Ä–µ–¥–Ω—è | –í–∏—Å–æ–∫–∞ |
| **–ù–∞–≤—á–∞–Ω–Ω—è** | –ù–µ–º–∞—î | Online learning | Probabilistic + RL |
| **–®–≤–∏–¥–∫—ñ—Å—Ç—å** | < 10ms | < 50ms | < 200ms |
| **–ü–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ—Å—Ç—å** | ‚úÖ –ü–æ–≤–Ω–∞ | ‚ö†Ô∏è –ß–∞—Å—Ç–∫–æ–≤–æ | ‚ùå –°–∫–ª–∞–¥–Ω–∞ |
| **–ü–æ—Ç—Ä–µ–±–∞ –≤ –¥–∞–Ω–∏—Ö** | –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ | –°–µ—Ä–µ–¥–Ω—è (100+ interactions) | –í–∏—Å–æ–∫–∞ (1000+ interactions) |
| **–ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è** | –ë–∞–∑–æ–≤–∞ | –î–æ–±—Ä–∞ | –í—ñ–¥–º—ñ–Ω–Ω–∞ |
| **–ü–æ—è—Å–Ω—é–≤–∞–Ω—ñ—Å—Ç—å** | ‚úÖ –ü–æ–≤–Ω–∞ | ‚ö†Ô∏è UCB scores | ‚ùå Black box |
| **Production ready** | ‚úÖ –¢–∞–∫ | üîÑ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è | ‚ùå Research only |

## –î–µ—Ç–∞–ª—å–Ω–∏–π –û–ø–∏—Å –†—ñ–≤–Ω—ñ–≤

### Level A: Rules-Based Adapter ‚ö°

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–æ–±–æ—Ç–∏:**
- Threshold-based rules –¥–ª—è difficulty adjustment
- User preferences –¥–ª—è format selection
- Response time –¥–ª—è tempo recommendations
- Topic mastery –¥–ª—è remediation

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```
IF avg_accuracy > 0.8 AND recent_accuracy > 0.85:
    difficulty ‚Üë (hard ‚Üí challenge)
ELIF avg_accuracy < 0.5 OR recent_accuracy < 0.4:
    difficulty ‚Üì (normal ‚Üí easy)
ELSE:
    maintain current difficulty

IF topic_mastery[topic] < 0.5:
    add to remediation_topics

format = user.preferred_format OR most_successful_format
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –®–≤–∏–¥–∫–∞ —Ä–æ–∑—Ä–æ–±–∫–∞ —Ç–∞ deployment
- ‚úÖ –ü–æ–≤–Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω—ñ—Å—Ç—å —Ç–∞ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ—Å—Ç—å
- ‚úÖ –õ–µ–≥–∫–µ debugging —Ç–∞ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —Ä—ñ—à–µ–Ω—å
- ‚úÖ –ù–µ –ø–æ—Ç—Ä–µ–±—É—î historical data

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –§—ñ–∫—Å–æ–≤–∞–Ω—ñ thresholds (–Ω–µ –∞–¥–∞–ø—Ç—É—é—Ç—å—Å—è)
- ‚ùå –ù–µ –≤—Ä–∞—Ö–æ–≤—É—î —Å–∫–ª–∞–¥–Ω—ñ –≤–∑–∞—î–º–æ–∑–≤'—è–∑–∫–∏ features
- ‚ùå –û–±–º–µ–∂–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è

---

### Level B: Contextual Bandit ü§ñ

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–æ–±–æ—Ç–∏:**
- LinUCB algorithm: Upper Confidence Bound
- 16 arms: 4 difficulty levels √ó 4 formats
- 17 context features –∑ user profile
- Online learning –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ—ó interaction

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```
context = extract_features(user_profile, recent_metrics)
  ‚Üí [avg_accuracy, topic_mastery_mean, response_time, ...]

For each arm a:
    Œ∏_a = A_a^(-1) * b_a
    UCB(a) = Œ∏_a^T * context + Œ± * sqrt(context^T * A_a^(-1) * context)

selected_arm = argmax(UCB)
  ‚Üí decode to (difficulty, format)

After user interaction:
    reward = 0.5*accuracy + 0.2*(1-normalized_time) + 0.15*(1-hints) + 0.15*engagement
    A_a ‚Üê A_a + context * context^T
    b_a ‚Üê b_a + reward * context
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –¥–æ—Å–≤—ñ–¥—É
- ‚úÖ Balance exploration/exploitation
- ‚úÖ –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ user
- ‚úÖ –ù–µ –ø–æ—Ç—Ä–µ–±—É—î pre-training (cold start OK)

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ö†Ô∏è –ü–æ—Ç—Ä–µ–±—É—î 100-200 interactions –¥–ª—è convergence
- ‚ö†Ô∏è Alpha tuning –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π
- ‚ö†Ô∏è –ú–µ–Ω—à–∞ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ—Å—Ç—å –Ω—ñ–∂ rules

---

### Level C: IRT/BKT + Policy Learning üî¨

**–ü—Ä–∏–Ω—Ü–∏–ø —Ä–æ–±–æ—Ç–∏:**
- IRT (Item Response Theory): –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è difficulty vs ability
- BKT (Bayesian Knowledge Tracing): tracking P(knowledge|evidence)
- Policy Network (RL): –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª–æ–≥–æ learning gain

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
```
# IRT: Estimate user ability Œ∏
P(correct|Œ∏, difficulty_i) = 1 / (1 + exp(-(Œ∏ - difficulty_i)))
Œ∏_user = estimate_ability(responses)

# BKT: Track knowledge state
P(L_t|evidence) = update_bayes(P(L_t-1), correctness, slip, guess)

# Policy: Select optimal action
state = [Œ∏_user, P(L), topic_mastery, ...]
action = policy_network(state)  # trained via PPO/A3C
  ‚Üí (next_difficulty, next_topic, next_format)

# Maximize long-term reward
reward = immediate_learning_gain + Œ≥ * future_mastery
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- ‚úÖ –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–æ –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ (IRT/BKT)
- ‚úÖ Probabilistic knowledge tracking
- ‚úÖ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—É
- ‚úÖ –ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è academic research

**–ù–µ–¥–æ–ª—ñ–∫–∏:**
- ‚ùå –°–∫–ª–∞–¥–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
- ‚ùå –ü–æ—Ç—Ä–µ–±—É—î –≤–µ–ª–∏–∫—ñ datasets (1000+ interactions)
- ‚ùå Computational overhead (RL training)
- ‚ùå Black box (–≤–∞–∂–∫–æ –ø–æ—è—Å–Ω–∏—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è)

---

## –ü–æ–µ—Ç–∞–ø–Ω–∏–π Roadmap

### ‚úÖ Phase 1: MVP (Completed)
- Rules-based adapter —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ
- Metrics collection working
- Profile aggregation functioning
- Production deployment ready

### üîÑ Phase 2: ML Enhancement (Week 7)
- LinUCB implementation
- Feature extraction (17 features)
- Reward computation
- A/B testing vs rules

### üîÆ Phase 3: Advanced Research (Future)
- IRT calibration
- BKT knowledge tracking
- Policy network training
- Academic evaluation

---

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ –ü—Ä–æ–µ–∫—Ç—ñ

**–ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω:**
```python
# backend/app/core/adaptation/engine.py
class AdaptationEngine:
    def __init__(self, strategy: AdaptationStrategy = RULES):
        if strategy == AdaptationStrategy.RULES:
            self.adapter = RulesAdapter()      # ‚úÖ Level A
        elif strategy == AdaptationStrategy.BANDIT:
            self.adapter = BanditAdapter()     # üîÑ Level B (Week 7)
        elif strategy == AdaptationStrategy.POLICY:
            self.adapter = PolicyAdapter()     # üîÆ Level C (Future)
```

**Config:**
```python
# backend/app/config.py
ADAPTATION_MODE = "rules"  # "rules" | "bandit" | "policy"
BANDIT_ALPHA = 1.5
BANDIT_MODEL_PATH = "models/linucb_bandit.npz"
```

---

## –î–∂–µ—Ä–µ–ª–∞

- **Level A**: `backend/app/core/adaptation/rules.py`
- **Level B**: `docs/weeks/week_7.md`, `docs/claude/ml_pipeline.md` (lines 235-417)
- **Level C**: `docs/claude/ml_pipeline.md` (lines 433-646)
- **Architecture**: `backend/app/core/adaptation/engine.py`
