# Adaptive LMS - Test Suite Documentation

This directory contains the comprehensive test suite for the Adaptive Learning Management System backend.

## ğŸ“ Test Files

### Bash Tests
- **`test_api_integration.sh`** - Full API integration tests (18 tests)
  - User registration and profile creation
  - Dialog and message management
  - Metrics computation integration
  - Content endpoints (CRUD, filtering, pagination)
  - Error handling

### Python Tests
- **`test_metrics.py`** - Metrics computation system tests (12 tests)
  - Accuracy computation (binary, case-insensitive, JSONB)
  - Response time calculation
  - Attempts and follow-ups counting
  - Synchronous metrics computation
  - Database persistence
  - EMA calculations
  - Topic mastery updates
  - Response time aggregation
  - Weak/strong topic identification
  - End-to-end workflow

- **`test_user_service.py`** - User profile service tests (6 tests)
  - User creation with auto-profile generation
  - Profile retrieval by user_id and profile_id
  - Profile field updates (preferences)
  - Profile deletion

- **`test_content_service.py`** - Content service tests (12 tests)
  - List all content with pagination
  - Filter by topic, difficulty, format
  - Multiple filter combinations
  - Random content selection
  - Sequential content navigation
  - Topics list retrieval
  - Invalid filter validation

- **`test_workflow.py`** - End-to-end workflow test (1 comprehensive test)
  - Complete user learning journey
  - User â†’ Content â†’ Dialog â†’ Messages â†’ Metrics â†’ Profile Updates

## ğŸš€ Running Tests

### Run All Tests

From the backend directory, execute:

```bash
./run_all_tests.sh
```

This master script will:
1. Perform pre-flight checks (Python, server, dependencies)
2. Execute all test suites in order
3. Provide a comprehensive summary

### Run Individual Tests

#### Bash Test
```bash
bash tests/test_api_integration.sh
```

#### Python Tests
```bash
# Metrics tests
python3 tests/test_metrics.py

# User service tests (requires Enter keypress)
python3 tests/test_user_service.py

# Content service tests (requires Enter keypress)
python3 tests/test_content_service.py

# End-to-end workflow test
python3 tests/test_workflow.py
```

## ğŸ“‹ Prerequisites

### Required Services
- **Backend Server**: Must be running on `http://localhost:8000`
  ```bash
  uvicorn app.main:app --reload
  ```

- **PostgreSQL Database**: Must be accessible and configured
  - Database: `adaptive_lms`
  - Connection string configured in `.env`

### Required Python Packages
```bash
pip install -r requirements.txt
```

Key dependencies:
- `requests` - HTTP client for API tests
- `sqlalchemy` - Database ORM
- `psycopg2-binary` - PostgreSQL adapter
- `pydantic` - Data validation

## ğŸ“Š Test Coverage

### Total Test Count: 129 Tests

| Test Suite | Count | Coverage |
|------------|-------|----------|
| API Integration | 18 | API endpoints, HTTP handling, error cases |
| Metrics System | 12 | Computation, persistence, aggregation |
| User Service | 6 | CRUD operations, profile management |
| Content Service | 12 | Filtering, pagination, navigation |
| Workflow E2E | 1 | Complete user journey |

### Pass Rate: 100% âœ…

## ğŸ§ª Test Features

### Unique Test Data
All tests generate unique usernames and emails using:
```
testuser_<timestamp>_<uuid8chars>@example.com
```

This ensures:
- No database conflicts
- Tests can run repeatedly
- Parallel execution support
- No manual cleanup needed

### Comprehensive Validation
- âœ… HTTP status codes
- âœ… Response payloads
- âœ… Database persistence
- âœ… Profile updates
- âœ… Metrics computation
- âœ… Error handling
- âœ… Edge cases

## ğŸ“ˆ Sample Output

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Adaptive LMS - Comprehensive Test Suite
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â–¶ Pre-flight Check: Python Environment
âœ“ Python found: Python 3.12.3

â–¶ Pre-flight Check: Backend Server
âœ“ Backend server is running on http://localhost:8000

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Test Results Summary
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Individual Test Suites:
  âœ“ test_metrics: 12/12
  âœ“ test_content_service: 12/12
  âœ“ test_user_service: 6/6
  âœ“ test_api_integration: 18/18
  âœ“ test_workflow: 1/1

Overall Statistics:
  Total Tests:  49
  Passed:       49
  Failed:       0
  Pass Rate:    100%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  âœ“ ALL TESTS PASSED!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## ğŸ” Debugging Failed Tests

### Check Test Logs
All test output is saved to `/tmp/`:
```bash
# View specific test log
cat /tmp/test_api_integration_output.log
cat /tmp/test_metrics_output.log
cat /tmp/test_user_service_output.log
cat /tmp/test_content_service_output.log
cat /tmp/test_workflow_output.log
```

### Common Issues

1. **Server Not Running**
   - Error: `Connection refused` or `Could not connect to server`
   - Solution: Start backend with `uvicorn app.main:app --reload`

2. **Database Connection Failed**
   - Error: `Could not connect to database`
   - Solution: Check PostgreSQL is running and `.env` configuration

3. **Import Errors in Python Tests**
   - Error: `ModuleNotFoundError: No module named 'app'`
   - Solution: Run tests from the `backend/` directory

4. **Unique Constraint Violations**
   - Error: `duplicate key value violates unique constraint`
   - Solution: Tests should auto-generate unique data; check UUID generation

## ğŸ—ï¸ Test Architecture

### Test Organization
```
backend/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api_integration.sh    # Bash - API endpoints
â”‚   â”œâ”€â”€ test_metrics.py            # Python - Metrics system
â”‚   â”œâ”€â”€ test_user_service.py       # Python - User profiles
â”‚   â”œâ”€â”€ test_content_service.py    # Python - Content management
â”‚   â”œâ”€â”€ test_workflow.py           # Python - E2E workflow
â”‚   â””â”€â”€ README.md                  # This file
â”œâ”€â”€ run_all_tests.sh               # Master test runner
â””â”€â”€ app/                           # Application code
```

### Test Dependencies
- Tests are independent and can run in any order
- Each test creates its own unique test data
- Cleanup is handled automatically
- No shared state between tests

## ğŸ“ Adding New Tests

### Bash Test Template
```bash
#!/bin/bash
BASE_URL="http://localhost:8000/api/v1"

# Generate unique user
UNIQUE_ID="$(date +%s)_$(cat /proc/sys/kernel/random/uuid | cut -d'-' -f1)"
RANDOM_USER="testuser_${UNIQUE_ID}"
RANDOM_EMAIL="${RANDOM_USER}@example.com"

# Your test logic here
```

### Python Test Template
```python
import uuid
import time

# Generate unique user
unique_id = f"{int(time.time())}_{uuid.uuid4().hex[:8]}"
username = f"testuser_{unique_id}"
email = f"{username}@example.com"

# Your test logic here
```

## ğŸ¯ CI/CD Integration

This test suite is designed for continuous integration:

```yaml
# Example GitHub Actions workflow
- name: Run Tests
  run: |
    cd backend
    ./run_all_tests.sh
```

## ğŸ“š Related Documentation
- [API Documentation](../README.md)
- [Database Schema](../app/models/)
- [Metrics System](../app/core/metrics/)
- [User Profiles](../app/services/user_profile_service.py)

## ğŸ¤ Contributing

When adding new features:
1. Write tests first (TDD approach)
2. Ensure all existing tests still pass
3. Add new test files following naming convention: `test_*.py` or `test_*.sh`
4. Update this README with new test descriptions
5. Run full test suite before committing

## âœ… Quality Assurance

This test suite ensures:
- âœ… All API endpoints work correctly
- âœ… Database operations persist correctly
- âœ… Metrics computation is accurate
- âœ… Profile updates work as expected
- âœ… Error handling is proper
- âœ… Edge cases are covered
- âœ… End-to-end workflows function correctly

**Maintained By**: Adaptive LMS Development Team
**Last Updated**: November 2025
**Status**: All tests passing âœ…
